{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the intensity of a z-stack, save single channel file for smFISH\n",
    "# 10 December 2018\n",
    "# \n",
    "#  Select the appropriate channel\n",
    "#  Measure average intensity of each image\n",
    "#  Save measurements as an array\n",
    "#  Calculate stack mean from the array,\n",
    "#  calculate the difference between the mean intensity of each frame and the stack mean,\n",
    "#  add the difference from each frame.\n",
    "#  To normalise an entire dataset, run the script and note the dataset mean intensity from the last file,\n",
    "#  then replace stackMean in line 74 with the dataset mean intensity value\n",
    "# \n",
    "#  Call from the command line with the following script:\n",
    "# fiji --headless --console -macro ~/src/FIJI_macros/zstack_normalisation_v7_cmndln.ijm /path/to/directory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def main(argList):\n",
    "    \n",
    "    parser= argparse.ArgumentParser()\n",
    "    \n",
    "    # specify the FQ centroid files and co-detection distance threshold in nm\n",
    "    parser.add_argument('-indir', help='FQ_spots file for reference channel')\n",
    "    parser.add_argument('-outdir', help='FQ_spots file for co-detection')\n",
    "    #parser.add_argument('-threshold', help='distance to calculate nearest neighbor')\n",
    "        \n",
    "    args= parser.parse_args(args=argList)\n",
    "\n",
    "    infiles = os.listdir(args.indir)\n",
    "\n",
    "    # create list to store data\n",
    "    filename = []\n",
    "    co_detect = []\n",
    "    ch1_spots = []\n",
    "    ch2_spots = []\n",
    "    \n",
    "    # loop through files\n",
    "    for i in infiles:\n",
    "        print \"processing\", i\n",
    "        \n",
    "        ref_file = os.path.join(args.ch1_indir,i)\n",
    "        print \"processing\", i\n",
    "        ref_file = pd.read_csv(ref_file, sep='\\t', header=18)\n",
    "        #ref_file = ref_file[~ref_file.Pos_Y.str.contains(\"SPOTS_END\")]\n",
    "        targ_file = os.path.join(args.ch2_indir, i)\n",
    "        targ_file = pd.read_csv(targ_file, sep='\\t', header=18)\n",
    "        #targ_file = targ_file[~targ_file.Pos_Y.str.contains(\"SPOTS_END\")]\n",
    "\n",
    "        # get centroid coordinates\n",
    "        xpos_ref = ref_file['Pos_X']\n",
    "        ypos_ref = ref_file['Pos_Y']\n",
    "        zpos_ref = ref_file['Pos_Z']\n",
    "\n",
    "        xpos_targ = targ_file['Pos_X']\n",
    "        ypos_targ = targ_file['Pos_Y']\n",
    "        zpos_targ = targ_file['Pos_Z']\n",
    "\n",
    "        # convert data into a numpy array\n",
    "        target_df = np.column_stack((xpos_targ,ypos_targ,zpos_targ))\n",
    "        #target_df = [target_df.Pos_Y.str.contains(\"SPOTS_END\") == False]\n",
    "\n",
    "        # create list to store amplitude of co-detected spot, ref/targ ratio, and target distance\n",
    "        targ_amp = []\n",
    "        targ_dist = []\n",
    "\n",
    "        for index, row in ref_file.iterrows():\n",
    "\n",
    "            # get 3D position from X,Y,Z position columns\n",
    "            pt  = row['Pos_X'], row['Pos_Y'], row['Pos_Z']\n",
    "            print pt\n",
    "            # find nearest neighbor and calculate distance\n",
    "            distance,index = spatial.KDTree(target_df).query(pt)\n",
    "\n",
    "            # add nearest neighbor amp and dist to a list\n",
    "            targ_amp.append(targ_file['AMP'].iloc[index])\n",
    "            targ_dist.append(distance)\n",
    "\n",
    "        # add lists to ref_file\n",
    "        ref_file['target_amp'] = targ_amp\n",
    "        ref_file['r_t_ratio'] = ref_file['AMP'].div(targ_amp)\n",
    "        ref_file['targ_dist'] = targ_dist\n",
    "\n",
    "        # calculate co-detection percentage and add it to list\n",
    "        codetect = 100 * (float(len(ref_file[ref_file.targ_dist <                       \n",
    "            float(args.threshold)])))/(float(len(ref_file.index)))\n",
    "        co_detect.append(codetect)\n",
    "\n",
    "        # calculate number of spots and add to list, with filename\n",
    "        ch1_spots.append(len(ref_file))\n",
    "        ch2_spots.append(len(targ_file))\n",
    "        filename.append(i)\n",
    "\n",
    "        # write ref_file to csv\n",
    "        #ref_file.to_csv('test_list.csv', index=False)\n",
    "    \n",
    "    # add data to dataframe and save\n",
    "    df = pd.DataFrame({'filename':filename,'co_detect%':co_detect, 'ch1_spots':ch1_spots, 'ch2_spots':ch2_spots})\n",
    "    df = df[['filename', 'co_detect%', 'ch1_spots', 'ch2_spots']]\n",
    "    df.to_csv('codetection_stats.csv', index=False)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main(sys.argv[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
